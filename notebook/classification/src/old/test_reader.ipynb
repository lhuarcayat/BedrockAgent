{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e1cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (1.38.21)\n",
      "Requirement already satisfied: PyPDF2 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: pydantic in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (2.11.5)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.21 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from boto3) (1.38.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from boto3) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.21->boto3) (1.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/lannister-applying/projects/applying/par_servicios/poc_bedrock/notebook/env/lib/python3.12/site-packages (from pydantic) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 PyPDF2 pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f604c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, io, base64, re, datetime\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "# from pypdf import PdfReader, PdfWriter\n",
    "from PyPDF2 import PdfWriter, PdfReader, PdfFileMerger\n",
    "from botocore.config import Config\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing   import Literal, Optional\n",
    "\n",
    "# (Optional) customize timeouts if you found you needed it:\n",
    "cfg = Config(connect_timeout=30, read_timeout=300)\n",
    "session = boto3.Session(profile_name=\"par_servicios\")\n",
    "# Create the Bedrock client:\n",
    "region = \"us-east-1\"\n",
    "bedrock = session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=region,\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5489c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_to_read):\n",
    "    with open(file_to_read, \"rb\") as document:\n",
    "        raw = document.read()\n",
    "        return raw\n",
    "\n",
    "\n",
    "def markdown_to_plain(md_path: Path) -> str:\n",
    "    \"\"\"Return a prompt string with most Markdown chrome removed.\"\"\"\n",
    "    text = md_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # 1) Remove HTML comments (often used for model delimiters)\n",
    "    text = re.sub(r\"<!--.*?-->\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # 2) Drop <details> … </details> blocks (examples already in your codebase)\n",
    "    text = re.sub(r\"<details>.*?</details>\", \"\", text,\n",
    "                  flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # 3) Remove fenced-code blocks ```…``` (they tend to confuse the model)\n",
    "    text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # 4) Strip Markdown headings, bold, italics, tables\n",
    "    text = re.sub(r\"^#+\\s*\", \"\", text, flags=re.MULTILINE)      # headings\n",
    "    text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"\\1\", text)               # bold → plain\n",
    "    text = re.sub(r\"__(.+?)__\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\*(.+?)\\*\",  r\"\\1\", text)                  # italics\n",
    "    text = re.sub(r\"_([^_]+)_\", r\"\\1\", text)\n",
    "    text = re.sub(r\"^\\|.*\\|\\s*$\", \"\", text, flags=re.MULTILINE)  # tables rows\n",
    "\n",
    "    # 5) Collapse multiple blank lines\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def set_model_params(max_tokens=300, top_p=0.1, temperature=0.3):\n",
    "    return {\n",
    "        \"maxTokens\":    max_tokens,\n",
    "        \"topP\":         top_p,\n",
    "        \"temperature\":  temperature\n",
    "    }\n",
    "\n",
    "\n",
    "def invoke_nova(model_id: str, messages: list, inference_cfg: dict):\n",
    "    payload = {\n",
    "        \"messages\":        messages,\n",
    "        \"inferenceConfig\": inference_cfg\n",
    "    }\n",
    "    resp = bedrock.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(payload).encode(\"utf-8\")\n",
    "    )\n",
    "    return json.loads(resp[\"body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ef53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_ROOT = Path(\"../../shared/file_examples\")\n",
    "S3_BUCKET  = \"par-servicios-docs\"\n",
    "\n",
    "def create_messages(prompt: str, pdf_bytes: bytes):\n",
    "\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # 1) prompt about what you want done\n",
    "            {\"text\": prompt},\n",
    "\n",
    "            # 2) the PDF itself, under a \"document\" key:\n",
    "            {\n",
    "                \"document\": {\n",
    "                    \"name\":   \"document_to_evaluate.pdf\",  # an arbitrary label\n",
    "                    \"format\": \"pdf\",                       # file format\n",
    "                    \"source\": {\n",
    "                        # \"bytes\": pdf_bytes                # raw bytes → SDK handles the rest\n",
    "                        \"bytes\": base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return [msg]\n",
    "\n",
    "def add_now_process(folder_path):\n",
    "    now_process = (\n",
    "        f\"\\nNOW PROCESS:\\n\"\n",
    "        f\"Folder path: `{folder_path}`\\n\"\n",
    "        f\"Extracted text follows the PDF below.\"\n",
    "    )\n",
    "\n",
    "    return now_process\n",
    "\n",
    "\n",
    "def build_folder_path(pdf_path: Path, use_s3: bool = False) -> str:\n",
    "    rel_parts = pdf_path.relative_to(LOCAL_ROOT).parts[:-1]   # drop filename\n",
    "    key = \"/\".join(rel_parts)                                 # ACC/800216686\n",
    "    return f\"s3://{S3_BUCKET}/{key}\" if use_s3 else f\"file_examples/{key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3268208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_pdf_page(file_name):\n",
    "    inputpdf = PdfReader(open(file_name, \"rb\"))\n",
    "    first_page = inputpdf.pages[0]\n",
    "    writer = PdfWriter()\n",
    "    writer.add_page(first_page)\n",
    "    buffer = io.BytesIO()\n",
    "    writer.write(buffer)\n",
    "    return buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b15a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalise(raw_obj: dict, *, file_path: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    • rename documenttype → document_type (case-insensitive)\n",
    "    • ensure document_number and path exist (fallbacks)\n",
    "    • keep snippet only if present\n",
    "    \"\"\"\n",
    "    norm = {k.lower(): v for k, v in raw_obj.items()}          # case-fold keys\n",
    "\n",
    "    # key mapping\n",
    "    if \"documenttype\" in norm and \"document_type\" not in norm:\n",
    "        norm[\"document_type\"] = norm.pop(\"documenttype\")\n",
    "\n",
    "    # fallback values\n",
    "    if \"document_number\" not in norm:\n",
    "        # try to derive from the file path    e.g.  .../800035887/...\n",
    "        if file_path:\n",
    "            m = re.search(r\"/(\\d{6,})/\", file_path)\n",
    "            norm[\"document_number\"] = m.group(1) if m else \"UNKNOWN\"\n",
    "        else:\n",
    "            norm[\"document_number\"] = \"UNKNOWN\"\n",
    "\n",
    "    if \"path\" not in norm and file_path:\n",
    "        norm[\"path\"] = file_path\n",
    "    elif \"path\" not in norm:\n",
    "        norm[\"path\"] = \"UNKNOWN\"\n",
    "\n",
    "    return norm\n",
    "\n",
    "def _strip_fences(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes ```json ... ``` or ``` ... ``` even if the opening fence is\n",
    "    immediately followed by '{'.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    # opening fence\n",
    "    text = re.sub(r'^```(?:json)?', '', text, flags=re.IGNORECASE).lstrip()\n",
    "    # closing fence\n",
    "    text = re.sub(r'```$', '', text).rstrip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab824fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "# 1)  Pydantic schema – guarantees we got the 5 required keys\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "class ClassMeta(BaseModel):\n",
    "    document_number: str\n",
    "    document_type:   Literal[\"person\", \"company\"]\n",
    "    category:        Literal[\"CERL\", \"CECRL\", \"RUT\", \"RUB\", \"ACC\",\n",
    "                            \"BLANK\", \"LINK_ONLY\"]\n",
    "    path:            str\n",
    "    text: Optional[str]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# 2)  Pull the assistant’s line of JSON out of Bedrock’s response\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def _extract_text(resp_json: dict) -> str:\n",
    "    \"\"\"\n",
    "    Bedrock Nova returns:\n",
    "        {\"output\":{\"message\":{\"content\":[{\"text\":\"...\"}]}}}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return resp_json[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        raise RuntimeError(\"Unexpected response shape from Bedrock\") from None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# 3)  Validate + return a ClassMeta instance (raises on error)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def parse_classification(resp_json: dict, *, pdf_path: str | None = None) -> ClassMeta:\n",
    "    raw_text = _extract_text(resp_json)\n",
    "    raw_text = _strip_fences(raw_text)          # remove ```json … ```\n",
    "\n",
    "    try:\n",
    "        raw_obj = json.loads(raw_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # last-chance: grab first '{' … last '}'\n",
    "        m1, m2 = raw_text.find(\"{\"), raw_text.rfind(\"}\")\n",
    "        if m1 != -1 and m2 != -1:\n",
    "            raw_obj = json.loads(raw_text[m1:m2+1])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Assistant did not return JSON: {e}\") from None\n",
    "\n",
    "    patched = _normalise(raw_obj, file_path=pdf_path)\n",
    "\n",
    "    return ClassMeta.model_validate(patched)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# 4)  Build the payload that Phase-2 expects\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def build_payload(meta: ClassMeta) -> dict:\n",
    "    return {\n",
    "        \"path\":            meta.path,\n",
    "        \"result\":          meta.model_dump(mode=\"json\"),   # dict\n",
    "        \"document_type\":   meta.document_type,\n",
    "        \"document_number\": meta.document_number,\n",
    "        \"category\":        meta.category\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96705d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(response, output_path=\"response_indented.json\", indent=2):\n",
    "    \"\"\"\n",
    "    Dumps `response` to JSON at `output_path`.\n",
    "    If necessary, creates parent directories.\n",
    "    Catches and reports serialization errors.\n",
    "    \"\"\"\n",
    "    out = Path(output_path)\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(response, f, indent=indent, ensure_ascii=False)\n",
    "        print(f\"✅ Saved JSON to {out}\")\n",
    "    except TypeError as e:\n",
    "        print(f\"⚠️ Could not JSON-serialize response: {e}\")\n",
    "        # Fall back to writing the raw repr\n",
    "        with out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(repr(response))\n",
    "        print(f\"🔧 Wrote raw Python repr to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33bdc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(action: str) -> str:\n",
    "    mapping = {\n",
    "        \"clasification\": Path(\"./instructions/clasification.txt\")\n",
    "    }\n",
    "    fn = mapping.get(action, Path(\"./instructions/clasification.txt\"))\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7e546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = Path(\"../../shared/file_examples/CERL/860006752/22_CamCom_2020-02-28.pdf\")\n",
    "# pdf_path = Path(\"../../shared/file_examples/ACC/800216686/231_CA_2020-02-29.pdf\")\n",
    "folder_path = build_folder_path(pdf_path)\n",
    "first_page = get_first_pdf_page(pdf_path)\n",
    "# file_to_read  = read_document(first_page)\n",
    "file_to_read = first_page\n",
    "\n",
    "clasification_prompt_raw = get_instructions(\"clasification\")\n",
    "base_prompt = markdown_to_plain(clasification_prompt_raw)\n",
    "clasification_prompt = base_prompt + add_now_process(folder_path)\n",
    "\n",
    "messages = create_messages(clasification_prompt, file_to_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99eb8584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved JSON to outputs/classification/prompt_txt/vcerl_9/response_model_test_vcerl_9.json\n",
      "✅ Saved JSON to outputs/classification/prompt_txt/vcerl_9/meta_vcerl_9.json\n",
      "✅ Saved JSON to outputs/classification/prompt_txt/vcerl_9/payload_vcerl_9.json\n"
     ]
    }
   ],
   "source": [
    "modelId = \"us.amazon.nova-pro-v1:0\"\n",
    "temperature = 0.1\n",
    "top_p = 0.9\n",
    "max_tokens = 8192\n",
    "\n",
    "cfg = set_model_params(max_tokens, top_p, temperature)\n",
    "version = \"cerl_9\"\n",
    "folder_route = f\"outputs/classification/prompt_txt/v{version}\"\n",
    "# folder_route = \"outputs/prompt_md\"\n",
    "\n",
    "resp_json = invoke_nova(modelId, messages, cfg)\n",
    "save_to_json(resp_json, f\"{folder_route}/response_model_test_v{version}.json\", 2)\n",
    "\n",
    "meta    = parse_classification(resp_json, pdf_path = str(pdf_path) )\n",
    "payload = build_payload(meta)\n",
    "save_to_json(meta.model_dump(), f\"{folder_route}/meta_v{version}.json\",   2)\n",
    "save_to_json(payload, f\"{folder_route}/payload_v{version}.json\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72eade8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Document Extraction Testing Framework\n\nThis notebook uses the new YAML-based testing framework for document extraction validation.\n\n## Features:\n- **YAML-only configuration**: Everything controlled from `config/test_config.yaml`\n- **Real Bedrock integration**: Uses production-grade shared functions\n- **3 test cases**: Field accuracy, blank detection, count validation\n- **Multiple document types**: CECRL, CERL, RUT, RUB, ACC (when available)\n- **Hierarchical controls**: Enable/disable at all levels\n\n## Usage:\n- **No code changes needed**: Modify only `config/test_config.yaml`\n- **Run any test combination**: Controlled by YAML configuration\n- **Compare prompt versions**: Specify versions to test in YAML"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# NEW YAML-BASED FRAMEWORK SETUP\n# =============================================================================\n\nimport sys\nsys.path.append('src')\n\nfrom test_manager import TestManager\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\n\nprint(\"ğŸš€ Loading new YAML-based testing framework...\")\nprint(\"ğŸ“ Configuration file: config/test_config.yaml\")\nprint(\"ğŸ”§ No hardcoded test data - everything in YAML!\")\n\n# Initialize test manager - everything loads from YAML\ntm = TestManager(\"config/test_config.yaml\")\nprint(\"âœ… Test Manager initialized with YAML configuration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“‹ Current Configuration\n\nSee what tests are enabled and will be executed:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show execution plan - what will actually run\nprint(\"ğŸ“‹ Current Execution Plan:\")\ntm.show_execution_plan()\n\nprint(\"\\nğŸ“Š Enabled Test Summary:\")\nenabled_tests = tm.get_enabled_tests()\nfor test_case, documents in enabled_tests.items():\n    print(f\"  {test_case}: {len(documents)} documents\")\n    for doc in documents[:3]:  # Show first 3\n        print(f\"    - {doc}\")\n    if len(documents) > 3:\n        print(f\"    ... and {len(documents) - 3} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ§ª Run Field Accuracy Test\n\nTest specific field values that should be correct (main test case):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run field accuracy test - main test case\nprint(\"ğŸ§ª Running Field Accuracy Test...\")\nprint(\"ğŸ¯ Tests: nationality fixes, name field corrections, field accuracy\\n\")\n\nresults = tm.run_test_case(\"field_accuracy_test\")\ntm.show_results_summary(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸš€ Run All Enabled Tests\n\nRun all enabled test cases (controlled by YAML configuration):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run all enabled tests\nprint(\"ğŸš€ Running All Enabled Tests...\")\nprint(\"ğŸ“Š Testing multiple prompt versions, documents, and validation scenarios\\n\")\n\nall_results = tm.run_all_enabled()\ntm.show_results_summary(all_results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## âš™ï¸ Configuration Examples\n\nThe framework is controlled entirely by `config/test_config.yaml`. Here are common configuration patterns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display common configuration examples\nprint(\"âš™ï¸ Common Configuration Patterns:\")\nprint(\"=\"*50)\n\nprint(\"\\n1ï¸âƒ£ Test Only CECRL Documents:\")\nprint(\"\"\"\nsettings:\n  CECRL: true\n  CERL: false\n  RUT: false\n\"\"\")\n\nprint(\"\\n2ï¸âƒ£ Compare Specific Prompt Versions:\")\nprint(\"\"\"\ntest_cases:\n  field_accuracy_test:\n    prompts_to_test: [\"v2.1.0\", \"v2.2.1\"]\n\"\"\")\n\nprint(\"\\n3ï¸âƒ£ Focus on Specific Test Case:\")\nprint(\"\"\"\ncategories:\n  field_accuracy:\n    enabled: true\n  blank_detection:\n    enabled: false\n  count_validation:\n    enabled: false\n\"\"\")\n\nprint(\"\\n4ï¸âƒ£ Test Single Document:\")\nprint(\"\"\"\ndocuments:\n  us_passport_venezuela:\n    enabled: true\n  others:\n    enabled: false\n\"\"\")\n\nprint(\"\\nğŸ“ To change configuration: Edit config/test_config.yaml and re-run cells!\")\nprint(\"ğŸ”„ No notebook code changes needed - everything is in YAML!\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## ğŸ“Š Test Case Details\n\nThe framework supports 3 core test cases:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Show detailed information about test cases\nprint(\"ğŸ“Š Framework Test Cases:\")\nprint(\"=\"*40)\n\nprint(\"\\n1ï¸âƒ£ Field Accuracy Test:\")\nprint(\"   ğŸ¯ Purpose: Test wrong/incomplete/swapped field data\")\nprint(\"   âœ… Validation: Compare extracted vs expected specific values\")\nprint(\"   ğŸ“‹ Example: nationality should be 'Venezuela' not 'Estados Unidos'\")\n\nprint(\"\\n2ï¸âƒ£ Blank Detection Test:\")\nprint(\"   ğŸ¯ Purpose: Document has data but model returns blank/empty\")\nprint(\"   âœ… Validation: Uses schema to check minimum required fields extracted\")\nprint(\"   ğŸ“‹ Example: Should extract firstName but model returns empty\")\n\nprint(\"\\n3ï¸âƒ£ Count Validation Test:\")\nprint(\"   ğŸ¯ Purpose: Should find N entities but finds different count\")\nprint(\"   âœ… Validation: Count items in arrays (like relatedParties)\")\nprint(\"   ğŸ“‹ Example: Should find 5 people, found 3\")\n\nprint(\"\\nğŸ”§ Current Status:\")\nconfig = tm.config_loader\ncategories = config.get_categories()\nfor cat_name, cat_info in categories.items():\n    status = \"âœ… ENABLED\" if cat_info.get('enabled', False) else \"âŒ DISABLED\"\n    print(f\"   {cat_name}: {status}\")\n\nprint(\"\\nğŸ“ To enable/disable: Modify categories section in test_config.yaml\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## ğŸ¯ Framework Features\n\nKey advantages of the new YAML-based approach:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Show framework capabilities and status\nprint(\"ğŸ¯ Framework Features & Status:\")\nprint(\"=\"*45)\n\nprint(\"\\nâœ… YAML-Only Configuration\")\nprint(\"   ğŸ“ File: config/test_config.yaml\")\nprint(\"   ğŸ”§ No hardcoded test data in notebooks\")\n\nprint(\"\\nâœ… Real Bedrock Integration\")\nprint(\"   ğŸ¤– Model: us.amazon.nova-pro-v1:0\")\nprint(\"   ğŸ“¡ Uses production shared functions\")\nprint(\"   ğŸ’¾ Real S3 document downloads\")\n\nprint(\"\\nâœ… Hierarchical Enable/Disable\")\nprint(\"   ğŸ¢ Document types: CECRL, CERL, RUT, RUB, ACC\")\nprint(\"   ğŸ“‚ Categories: field_accuracy, blank_detection, count_validation\")\nprint(\"   ğŸ“„ Individual documents and test cases\")\n\nprint(\"\\nâœ… Multi-Version Prompt Testing\")\nsettings = tm.config_loader.get_settings()\ntest_cases = tm.config_loader.get_test_cases()\nversions = test_cases.get('field_accuracy_test', {}).get('prompts_to_test', [])\nprint(f\"   ğŸ“ Available versions: {versions}\")\n\nprint(\"\\nâœ… Schema-Based Validation\")\nprint(\"   ğŸ” Field accuracy: Compare extracted vs expected values\")\nprint(\"   ğŸš« Blank detection: Check required fields extracted\")  \nprint(\"   ğŸ”¢ Count validation: Verify entity counts\")\n\nprint(\"\\nğŸ“Š Current Configuration:\")\nsettings = tm.config_loader.get_settings()\nfor doc_type in ['CECRL', 'CERL', 'RUT', 'RUB', 'ACC']:\n    status = \"âœ…\" if settings.get(doc_type, False) else \"âŒ\"\n    print(f\"   {status} {doc_type}\")\n\nprint(\"\\nğŸ® Next Steps:\")\nprint(\"   1. Modify config/test_config.yaml for different scenarios\")\nprint(\"   2. Re-run notebook cells to test changes\")\nprint(\"   3. Add new document types when test data available\")\nprint(\"   4. No code changes needed - pure YAML configuration!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest comparison report\n",
    "report_path = COMPARISON_DIR / \"comparison_report.json\"\n",
    "if report_path.exists():\n",
    "    with open(report_path) as f:\n",
    "        report = json.load(f)\n",
    "\n",
    "    print_summary(report)\n",
    "else:\n",
    "    print(\"âŒ No comparison report found. Run tests first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Detailed Field Analysis\n",
    "\n",
    "Examine specific field changes in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of field changes\n",
    "def show_detailed_changes(document_key: str):\n",
    "    \"\"\"Show detailed field changes for a specific document.\"\"\"\n",
    "    report_path = COMPARISON_DIR / \"comparison_report.json\"\n",
    "    if not report_path.exists():\n",
    "        print(\"âŒ No comparison report found. Run tests first.\")\n",
    "        return\n",
    "\n",
    "    with open(report_path) as f:\n",
    "        report = json.load(f)\n",
    "\n",
    "    if document_key not in report[\"document_comparisons\"]:\n",
    "        print(f\"âŒ Document '{document_key}' not found in report\")\n",
    "        return\n",
    "\n",
    "    comparison = report[\"document_comparisons\"][document_key]\n",
    "\n",
    "    print(f\"\\nğŸ” Detailed Analysis: {comparison['document_name']}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(f\"ğŸ“„ Description: {comparison['description']}\")\n",
    "    print(f\"ğŸ¯ Expected fixes: {comparison['expected_fixes']}\")\n",
    "\n",
    "    print(\"\\nğŸ“Š Field-by-Field Comparison:\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    for field, change_data in comparison[\"field_changes\"].items():\n",
    "        if change_data.get(\"changed\", False):\n",
    "            print(f\"ğŸ”„ {field}:\")\n",
    "            print(f\"   v2.0.0: '{change_data['old_value']}'\")\n",
    "            print(f\"   v2.1.0: '{change_data['new_value']}'\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"âœ… {field}: '{change_data['value']}' (unchanged)\")\n",
    "\n",
    "# Example usage:\n",
    "show_detailed_changes(\"us_passport_venezuela\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ File Outputs\n\nAll test results are saved to:\n\n- `outputs/batch_results.json` - Raw test results\n- `outputs/comparison/comparison_report.json` - Formatted comparison report  \n- `outputs/before/` - v2.0.0 results by document\n- `outputs/after/` - v2.1.0 results by document\n- `outputs/test_log.txt` - Execution logs\n\n## ğŸ”§ Adding New Test Documents\n\nTo test additional documents, modify `TEST_DOCUMENTS` in `config.py`:\n\n```python\nTEST_DOCUMENTS[\"new_document\"] = {\n    \"name\": \"Description\",\n    \"description\": \"What should be fixed\",\n    \"s3_path\": \"s3://bucket/path/file.pdf\", \n    \"expected_fixes\": {\"field\": \"expected_value\"}\n}\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Document Extraction Testing Framework\n\nThis notebook uses the new YAML-based testing framework for document extraction validation.\n\n## Features:\n- **YAML-only configuration**: Everything controlled from `config/test_config.yaml`\n- **Real Bedrock integration**: Uses production-grade shared functions\n- **3 test cases**: Field accuracy, blank detection, count validation\n- **Multiple document types**: CECRL, CERL, RUT, RUB, ACC (when available)\n- **Hierarchical controls**: Enable/disable at all levels\n\n## Usage:\n- **No code changes needed**: Modify only `config/test_config.yaml`\n- **Run any test combination**: Controlled by YAML configuration\n- **Compare prompt versions**: Specify versions to test in YAML"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# NEW YAML-BASED FRAMEWORK SETUP\n# =============================================================================\n\nimport sys\nsys.path.append('src')\n\nfrom test_manager import TestManager\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\n\nprint(\"🚀 Loading new YAML-based testing framework...\")\nprint(\"📁 Configuration file: config/test_config.yaml\")\nprint(\"🔧 No hardcoded test data - everything in YAML!\")\n\n# Initialize test manager - everything loads from YAML\ntm = TestManager(\"config/test_config.yaml\")\nprint(\"✅ Test Manager initialized with YAML configuration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📋 Current Configuration\n\nSee what tests are enabled and will be executed:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show execution plan - what will actually run\nprint(\"📋 Current Execution Plan:\")\ntm.show_execution_plan()\n\nprint(\"\\n📊 Enabled Test Summary:\")\nenabled_tests = tm.get_enabled_tests()\nfor test_case, documents in enabled_tests.items():\n    print(f\"  {test_case}: {len(documents)} documents\")\n    for doc in documents[:3]:  # Show first 3\n        print(f\"    - {doc}\")\n    if len(documents) > 3:\n        print(f\"    ... and {len(documents) - 3} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🧪 Run Field Accuracy Test\n\nTest specific field values that should be correct (main test case):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run field accuracy test - main test case\nprint(\"🧪 Running Field Accuracy Test...\")\nprint(\"🎯 Tests: nationality fixes, name field corrections, field accuracy\\n\")\n\nresults = tm.run_test_case(\"field_accuracy_test\")\ntm.show_results_summary(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🚀 Run All Enabled Tests\n\nRun all enabled test cases (controlled by YAML configuration):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run all enabled tests\nprint(\"🚀 Running All Enabled Tests...\")\nprint(\"📊 Testing multiple prompt versions, documents, and validation scenarios\\n\")\n\nall_results = tm.run_all_enabled()\ntm.show_results_summary(all_results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ⚙️ Configuration Examples\n\nThe framework is controlled entirely by `config/test_config.yaml`. Here are common configuration patterns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display common configuration examples\nprint(\"⚙️ Common Configuration Patterns:\")\nprint(\"=\"*50)\n\nprint(\"\\n1️⃣ Test Only CECRL Documents:\")\nprint(\"\"\"\nsettings:\n  CECRL: true\n  CERL: false\n  RUT: false\n\"\"\")\n\nprint(\"\\n2️⃣ Compare Specific Prompt Versions:\")\nprint(\"\"\"\ntest_cases:\n  field_accuracy_test:\n    prompts_to_test: [\"v2.1.0\", \"v2.2.1\"]\n\"\"\")\n\nprint(\"\\n3️⃣ Focus on Specific Test Case:\")\nprint(\"\"\"\ncategories:\n  field_accuracy:\n    enabled: true\n  blank_detection:\n    enabled: false\n  count_validation:\n    enabled: false\n\"\"\")\n\nprint(\"\\n4️⃣ Test Single Document:\")\nprint(\"\"\"\ndocuments:\n  us_passport_venezuela:\n    enabled: true\n  others:\n    enabled: false\n\"\"\")\n\nprint(\"\\n📝 To change configuration: Edit config/test_config.yaml and re-run cells!\")\nprint(\"🔄 No notebook code changes needed - everything is in YAML!\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 📊 Test Case Details\n\nThe framework supports 3 core test cases:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Show detailed information about test cases\nprint(\"📊 Framework Test Cases:\")\nprint(\"=\"*40)\n\nprint(\"\\n1️⃣ Field Accuracy Test:\")\nprint(\"   🎯 Purpose: Test wrong/incomplete/swapped field data\")\nprint(\"   ✅ Validation: Compare extracted vs expected specific values\")\nprint(\"   📋 Example: nationality should be 'Venezuela' not 'Estados Unidos'\")\n\nprint(\"\\n2️⃣ Blank Detection Test:\")\nprint(\"   🎯 Purpose: Document has data but model returns blank/empty\")\nprint(\"   ✅ Validation: Uses schema to check minimum required fields extracted\")\nprint(\"   📋 Example: Should extract firstName but model returns empty\")\n\nprint(\"\\n3️⃣ Count Validation Test:\")\nprint(\"   🎯 Purpose: Should find N entities but finds different count\")\nprint(\"   ✅ Validation: Count items in arrays (like relatedParties)\")\nprint(\"   📋 Example: Should find 5 people, found 3\")\n\nprint(\"\\n🔧 Current Status:\")\nconfig = tm.config_loader\ncategories = config.get_categories()\nfor cat_name, cat_info in categories.items():\n    status = \"✅ ENABLED\" if cat_info.get('enabled', False) else \"❌ DISABLED\"\n    print(f\"   {cat_name}: {status}\")\n\nprint(\"\\n📝 To enable/disable: Modify categories section in test_config.yaml\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 🎯 Framework Features\n\nKey advantages of the new YAML-based approach:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Show framework capabilities and status\nprint(\"🎯 Framework Features & Status:\")\nprint(\"=\"*45)\n\nprint(\"\\n✅ YAML-Only Configuration\")\nprint(\"   📁 File: config/test_config.yaml\")\nprint(\"   🔧 No hardcoded test data in notebooks\")\n\nprint(\"\\n✅ Real Bedrock Integration\")\nprint(\"   🤖 Model: us.amazon.nova-pro-v1:0\")\nprint(\"   📡 Uses production shared functions\")\nprint(\"   💾 Real S3 document downloads\")\n\nprint(\"\\n✅ Hierarchical Enable/Disable\")\nprint(\"   🏢 Document types: CECRL, CERL, RUT, RUB, ACC\")\nprint(\"   📂 Categories: field_accuracy, blank_detection, count_validation\")\nprint(\"   📄 Individual documents and test cases\")\n\nprint(\"\\n✅ Multi-Version Prompt Testing\")\nsettings = tm.config_loader.get_settings()\ntest_cases = tm.config_loader.get_test_cases()\nversions = test_cases.get('field_accuracy_test', {}).get('prompts_to_test', [])\nprint(f\"   📝 Available versions: {versions}\")\n\nprint(\"\\n✅ Schema-Based Validation\")\nprint(\"   🔍 Field accuracy: Compare extracted vs expected values\")\nprint(\"   🚫 Blank detection: Check required fields extracted\")  \nprint(\"   🔢 Count validation: Verify entity counts\")\n\nprint(\"\\n📊 Current Configuration:\")\nsettings = tm.config_loader.get_settings()\nfor doc_type in ['CECRL', 'CERL', 'RUT', 'RUB', 'ACC']:\n    status = \"✅\" if settings.get(doc_type, False) else \"❌\"\n    print(f\"   {status} {doc_type}\")\n\nprint(\"\\n🎮 Next Steps:\")\nprint(\"   1. Modify config/test_config.yaml for different scenarios\")\nprint(\"   2. Re-run notebook cells to test changes\")\nprint(\"   3. Add new document types when test data available\")\nprint(\"   4. No code changes needed - pure YAML configuration!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest comparison report\n",
    "report_path = COMPARISON_DIR / \"comparison_report.json\"\n",
    "if report_path.exists():\n",
    "    with open(report_path) as f:\n",
    "        report = json.load(f)\n",
    "\n",
    "    print_summary(report)\n",
    "else:\n",
    "    print(\"❌ No comparison report found. Run tests first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Detailed Field Analysis\n",
    "\n",
    "Examine specific field changes in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of field changes\n",
    "def show_detailed_changes(document_key: str):\n",
    "    \"\"\"Show detailed field changes for a specific document.\"\"\"\n",
    "    report_path = COMPARISON_DIR / \"comparison_report.json\"\n",
    "    if not report_path.exists():\n",
    "        print(\"❌ No comparison report found. Run tests first.\")\n",
    "        return\n",
    "\n",
    "    with open(report_path) as f:\n",
    "        report = json.load(f)\n",
    "\n",
    "    if document_key not in report[\"document_comparisons\"]:\n",
    "        print(f\"❌ Document '{document_key}' not found in report\")\n",
    "        return\n",
    "\n",
    "    comparison = report[\"document_comparisons\"][document_key]\n",
    "\n",
    "    print(f\"\\n🔍 Detailed Analysis: {comparison['document_name']}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(f\"📄 Description: {comparison['description']}\")\n",
    "    print(f\"🎯 Expected fixes: {comparison['expected_fixes']}\")\n",
    "\n",
    "    print(\"\\n📊 Field-by-Field Comparison:\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    for field, change_data in comparison[\"field_changes\"].items():\n",
    "        if change_data.get(\"changed\", False):\n",
    "            print(f\"🔄 {field}:\")\n",
    "            print(f\"   v2.0.0: '{change_data['old_value']}'\")\n",
    "            print(f\"   v2.1.0: '{change_data['new_value']}'\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"✅ {field}: '{change_data['value']}' (unchanged)\")\n",
    "\n",
    "# Example usage:\n",
    "show_detailed_changes(\"us_passport_venezuela\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📁 File Outputs\n\nAll test results are saved to:\n\n- `outputs/batch_results.json` - Raw test results\n- `outputs/comparison/comparison_report.json` - Formatted comparison report  \n- `outputs/before/` - v2.0.0 results by document\n- `outputs/after/` - v2.1.0 results by document\n- `outputs/test_log.txt` - Execution logs\n\n## 🔧 Adding New Test Documents\n\nTo test additional documents, modify `TEST_DOCUMENTS` in `config.py`:\n\n```python\nTEST_DOCUMENTS[\"new_document\"] = {\n    \"name\": \"Description\",\n    \"description\": \"What should be fixed\",\n    \"s3_path\": \"s3://bucket/path/file.pdf\", \n    \"expected_fixes\": {\"field\": \"expected_value\"}\n}\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}